{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1b8347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "# installing selenium\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384dc171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required drivers\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8162e0",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6547d1",
   "metadata": {},
   "source": [
    "# Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dbd8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e3ceb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webpage https://www.naukri.com\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2bb7882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"Data Analyst\" in \"Skill, Designation, Companies\" field\n",
    "designation=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[3]/div/div/div[1]/div/div/div/input')\n",
    "designation.send_keys('Data Analyst')\n",
    "\n",
    "#entering \"Bangalore\" in \"enter the location\" field\n",
    "location=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input')\n",
    "location.send_keys(\"Bangalore\")\n",
    "\n",
    "#clicking the Search button\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[3]/div/div/div[6]')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3df63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping job-title from the webpage\n",
    "job_title=[]\n",
    "for i in driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')[0:10]:\n",
    "    job_title.append(i.text)\n",
    "    \n",
    "# Scrapping job location from the webpage\n",
    "job_location=[]\n",
    "for i in driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]//span')[0:10]:\n",
    "    job_location.append(i.text)\n",
    "\n",
    "# Scrapping Company Name from the webpage\n",
    "company_name=[]\n",
    "for i in driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')[0:10]:\n",
    "    company_name.append(i.text)\n",
    "\n",
    "# Scrapping Experience from the webpage\n",
    "experience_required=[]\n",
    "for i in driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]//span')[0:10]:\n",
    "    experience_required.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57fadd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business &amp; Data Analyst- Assistant Manager</td>\n",
       "      <td>Bangalore/Bengaluru, Hyderabad/Secunderabad</td>\n",
       "      <td>State Street</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru(Old Madras Road)</td>\n",
       "      <td>KrazyBee</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Qualitest India Private Limited</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sr. Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, Pune</td>\n",
       "      <td>Global Indian School Education Services</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sr Clinical Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>LabCorp</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Master Data Management Business Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst - Python/Artificial Intelligence</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>iMindYourBusiness</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Asst Clinical Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>LabCorp</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lead Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>McAfee</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, Ahmedabad</td>\n",
       "      <td>milestone internet marketing pvt ltd</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Job_Title  \\\n",
       "0     Business & Data Analyst- Assistant Manager   \n",
       "1                            Senior Data Analyst   \n",
       "2                            Senior Data Analyst   \n",
       "3                               Sr. Data Analyst   \n",
       "4                       Sr Clinical Data Analyst   \n",
       "5        Master Data Management Business Analyst   \n",
       "6  Data Analyst - Python/Artificial Intelligence   \n",
       "7                     Asst Clinical Data Analyst   \n",
       "8                              Lead Data Analyst   \n",
       "9                                   Data Analyst   \n",
       "\n",
       "                                        Job_Location  \\\n",
       "0        Bangalore/Bengaluru, Hyderabad/Secunderabad   \n",
       "1               Bangalore/Bengaluru(Old Madras Road)   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                          Bangalore/Bengaluru, Pune   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                     Bangalore/Bengaluru, Ahmedabad   \n",
       "\n",
       "                              Company_Name Experience_required  \n",
       "0                             State Street             1-3 Yrs  \n",
       "1                                 KrazyBee             3-6 Yrs  \n",
       "2          Qualitest India Private Limited             5-8 Yrs  \n",
       "3  Global Indian School Education Services            6-11 Yrs  \n",
       "4                                  LabCorp             2-5 Yrs  \n",
       "5                                Accenture             6-8 Yrs  \n",
       "6                        iMindYourBusiness             0-2 Yrs  \n",
       "7                                  LabCorp             1-3 Yrs  \n",
       "8                                   McAfee             5-9 Yrs  \n",
       "9     milestone internet marketing pvt ltd             2-7 Yrs  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting scrapped data into DataFrame\n",
    "df=pd.DataFrame({'Job_Title':job_title,'Job_Location':job_location,'Company_Name':company_name,'Experience_required':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148cfc8d",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6091207",
   "metadata": {},
   "source": [
    "# Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be384c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a8a04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webpage https://www.naukri.com\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05548932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"Data Scientist\" in \"Skill, Designation, Companies\" field\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa3b1fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"Bangalore\" in \"enter the location\" field\n",
    "location=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input')\n",
    "location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72374692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking the Search button\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[3]/div/div/div[6]')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "316ed3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping job-title from the webpage\n",
    "job_title=[]\n",
    "for i in driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')[0:10]:\n",
    "    job_title.append(i.text)\n",
    "    \n",
    "# Scrapping job location from the webpage\n",
    "job_location=[]\n",
    "for j in driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]//span')[0:10]:\n",
    "    job_location.append(j.text)\n",
    "\n",
    "# Scrapping Company Name from the webpage\n",
    "company_name=[]\n",
    "for k in driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')[0:10]:\n",
    "    company_name.append(k.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2494ef4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lead ML Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Manager - EmTech - Machine Learning - P&amp;T</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>PwC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data scientist _Tata Consultancy Services(Tcs)</td>\n",
       "      <td>Bangalore/Bengaluru, Kochi/Cochin, Indore, New...</td>\n",
       "      <td>TATA CONSULTANCY SERVICES (TCS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Job||Job Opening For AI Technologist - Data Sc...</td>\n",
       "      <td>Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...</td>\n",
       "      <td>Wipro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hiring For DATA Scientist @ NTT DATA Business ...</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "      <td>NTT DATA Business Solutions Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tcs Hiring For Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Chennai, Mumbai (All Areas)</td>\n",
       "      <td>TATA CONSULTANCY SERVICES (TCS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Urgent Job Opening For AI Practitioner - Data ...</td>\n",
       "      <td>Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...</td>\n",
       "      <td>Wipro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist Python</td>\n",
       "      <td>Bangalore/Bengaluru, Hyderabad/Secunderabad</td>\n",
       "      <td>Conduent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Assistant Manager - Data Science</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune</td>\n",
       "      <td>CitiusTech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0                                  Lead ML Scientist   \n",
       "1   Senior Manager - EmTech - Machine Learning - P&T   \n",
       "2                   Analystics & Modeling Specialist   \n",
       "3     Data scientist _Tata Consultancy Services(Tcs)   \n",
       "4  Job||Job Opening For AI Technologist - Data Sc...   \n",
       "5  Hiring For DATA Scientist @ NTT DATA Business ...   \n",
       "6                      Tcs Hiring For Data Scientist   \n",
       "7  Urgent Job Opening For AI Practitioner - Data ...   \n",
       "8                              Data Scientist Python   \n",
       "9                   Assistant Manager - Data Science   \n",
       "\n",
       "                                        Job_Location  \\\n",
       "0                        Bangalore/Bengaluru, Mumbai   \n",
       "1  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...   \n",
       "2  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...   \n",
       "3  Bangalore/Bengaluru, Kochi/Cochin, Indore, New...   \n",
       "4  Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...   \n",
       "5  Bangalore/Bengaluru, Noida, Hyderabad/Secunder...   \n",
       "6   Bangalore/Bengaluru, Chennai, Mumbai (All Areas)   \n",
       "7  Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...   \n",
       "8        Bangalore/Bengaluru, Hyderabad/Secunderabad   \n",
       "9                  Bangalore/Bengaluru, Mumbai, Pune   \n",
       "\n",
       "                                  Company_Name  \n",
       "0                            Fractal Analytics  \n",
       "1                                          PwC  \n",
       "2                                    Accenture  \n",
       "3              TATA CONSULTANCY SERVICES (TCS)  \n",
       "4                                        Wipro  \n",
       "5  NTT DATA Business Solutions Private Limited  \n",
       "6              TATA CONSULTANCY SERVICES (TCS)  \n",
       "7                                        Wipro  \n",
       "8                                     Conduent  \n",
       "9                                   CitiusTech  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting scrapped data into DataFrame\n",
    "df=pd.DataFrame({'Job_Title':job_title,'Job_Location':job_location,'Company_Name':company_name})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca3bf6",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "\n",
    "You have to use the location and salary filter.\n",
    "\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "\n",
    "The task will be done as shown in the below steps:\n",
    "\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6226f6",
   "metadata": {},
   "source": [
    "# Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04b7baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4b18cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting web-page https://www.naukri.com/\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27a5339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"Data Scientist\" in \"Skill, Designation, Companies\" field\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "#clicking the Search button\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bba16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying location filter and selecting \"Delhi/NCR\" otion\n",
    "location=driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[5]/div[2]/div[3]/label/i')\n",
    "location.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2288b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying Salary filter and selecting \"3-6 lakhs\" otion\n",
    "salary=driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[6]/div[2]/div[2]/label/i')\n",
    "salary.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e7c312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping job-title from the webpage\n",
    "jtitle=[]\n",
    "for i in driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')[0:10]:\n",
    "    jtitle.append(i.text)\n",
    "    \n",
    "# Scrapping job-location from the webpage\n",
    "jloc=[]\n",
    "for j in driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]//span')[0:10]:\n",
    "    jloc.append(j.text)\n",
    "\n",
    "# Scrapping Company name from the webpage\n",
    "comp=[]\n",
    "for k in driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')[0:10]:\n",
    "    comp.append(k.text)\n",
    "    \n",
    "# Scrapping Experience required from the webpage\n",
    "exp=[]\n",
    "for l in  driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]//span')[0:10]:\n",
    "    exp.append(l.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c76dc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job||Job Opening For AI Technologist - Data Sc...</td>\n",
       "      <td>New Delhi, Hyderabad/Secunderabad, Pune, Chenn...</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist - Noida/Bangalore</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "      <td>EXL</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DigitalBCG GAMMA Data Scientist</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - Predictive Analytics</td>\n",
       "      <td>Noida, Mumbai, Chandigarh, Hyderabad/Secundera...</td>\n",
       "      <td>Confidential</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Optum</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chat-bot Developer / Data Scientist</td>\n",
       "      <td>Mumbai, New Delhi, Bangalore/Bengaluru</td>\n",
       "      <td>Big Seo Buzz</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>(WFH during Covid)</td>\n",
       "      <td>Feedback Infra</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist / Chat-bot Developer</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Big Seo Buzz</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru, Mumbai (All Ar...</td>\n",
       "      <td>Primo Hiring</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist For Healthcare Product team</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>SECUREKLOUD TECHNOLOGIES</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0  Job||Job Opening For AI Technologist - Data Sc...   \n",
       "1                   Data Scientist - Noida/Bangalore   \n",
       "2                    DigitalBCG GAMMA Data Scientist   \n",
       "3              Data Scientist - Predictive Analytics   \n",
       "4                                     Data Scientist   \n",
       "5                Chat-bot Developer / Data Scientist   \n",
       "6                                     Data Scientist   \n",
       "7                Data Scientist / Chat-bot Developer   \n",
       "8                  Data Scientist - Engine Algorithm   \n",
       "9         Data Scientist For Healthcare Product team   \n",
       "\n",
       "                                        Job_Location  \\\n",
       "0  New Delhi, Hyderabad/Secunderabad, Pune, Chenn...   \n",
       "1                         Noida, Bangalore/Bengaluru   \n",
       "2                     New Delhi, Bangalore/Bengaluru   \n",
       "3  Noida, Mumbai, Chandigarh, Hyderabad/Secundera...   \n",
       "4                                   Gurgaon/Gurugram   \n",
       "5             Mumbai, New Delhi, Bangalore/Bengaluru   \n",
       "6                                 (WFH during Covid)   \n",
       "7                                   Gurgaon/Gurugram   \n",
       "8  New Delhi, Bangalore/Bengaluru, Mumbai (All Ar...   \n",
       "9  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "\n",
       "               Company_Name Experience required  \n",
       "0                     Wipro            5-10 Yrs  \n",
       "1                       EXL            5-10 Yrs  \n",
       "2   Boston Consulting Group             2-5 Yrs  \n",
       "3              Confidential             1-6 Yrs  \n",
       "4                     Optum             2-7 Yrs  \n",
       "5              Big Seo Buzz             2-7 Yrs  \n",
       "6            Feedback Infra             2-4 Yrs  \n",
       "7              Big Seo Buzz             3-7 Yrs  \n",
       "8              Primo Hiring             1-3 Yrs  \n",
       "9  SECUREKLOUD TECHNOLOGIES             2-7 Yrs  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting scrapped data into DataFrame\n",
    "df=pd.DataFrame({'Job_Title':jtitle,'Job_Location':jloc,'Company_Name':comp,'Experience required':exp})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e7975",
   "metadata": {},
   "source": [
    "# Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b2c59c",
   "metadata": {},
   "source": [
    "\n",
    "# Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e7e2745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9f4f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webpage https://www.flipkart.com/\n",
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78ff5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing the log-in page window\n",
    "close=driver.find_element(By.XPATH,'/html/body/div[2]/div/div/button')\n",
    "close.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1191312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"sunglasses\" in \"search for products\" field\n",
    "product=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "product.send_keys('Sunglasses')\n",
    "\n",
    "#clicking the Search button\n",
    "search=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea3cffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping Brand detail from the webpage\n",
    "brand=[]\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]'):\n",
    "    brand.append(i.text)\n",
    "\n",
    "# Scrapping Product Description from the webpage\n",
    "pro1=[]\n",
    "for j in driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa _2-ICcC\"]'):\n",
    "    pro1.append(j.text)\n",
    "    \n",
    "pro2=[]\n",
    "for j in driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]'):\n",
    "    pro2.append(j.text)\n",
    "    \n",
    "pro_des=pro1+pro2\n",
    "\n",
    "# Scrapping Price from the webpage\n",
    "price=[]\n",
    "for k in driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]'):\n",
    "    price.append(k.text)\n",
    "\n",
    "# Scrapping Discount from the webpage\n",
    "dis=[]\n",
    "for l in driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]'):\n",
    "    dis.append(l.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a902f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Wayfarer ...</td>\n",
       "      <td>₹749</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Silver Kartz</td>\n",
       "      <td>UV Protection Clubmaster Sunglasses (53)</td>\n",
       "      <td>₹281</td>\n",
       "      <td>81% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹799</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹639</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹224</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (52)</td>\n",
       "      <td>₹252</td>\n",
       "      <td>90% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>UV Protection, Polarized Wayfarer Sunglasses (...</td>\n",
       "      <td>₹283</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Cat-eye S...</td>\n",
       "      <td>₹949</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹298</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>₹211</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹264</td>\n",
       "      <td>89% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart UV Protection Aviator Sunglasses (53)</td>\n",
       "      <td>₹1,229</td>\n",
       "      <td>38% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹639</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>UV Protection Oval Sunglasses (Free Size)</td>\n",
       "      <td>₹207</td>\n",
       "      <td>79% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PHENOMENAL</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (53)</td>\n",
       "      <td>₹267</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Aviator S...</td>\n",
       "      <td>₹999</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SHAAH COLLECTIONS</td>\n",
       "      <td>UV Protection, Polarized, Mirrored Rectangular...</td>\n",
       "      <td>₹195</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>Polarized, Riding Glasses Sports, Wrap-around ...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NuVew</td>\n",
       "      <td>UV Protection Aviator Sunglasses (57)</td>\n",
       "      <td>₹198</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Aviator S...</td>\n",
       "      <td>₹949</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (Free Size)</td>\n",
       "      <td>₹499</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (58)</td>\n",
       "      <td>₹323</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LIZA ANGEL</td>\n",
       "      <td>Riding Glasses, Night Vision Spectacle Sunglas...</td>\n",
       "      <td>₹199</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>john jacobs</td>\n",
       "      <td>UV Protection Round Sunglasses (53)</td>\n",
       "      <td>₹3,325</td>\n",
       "      <td>33% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹221</td>\n",
       "      <td>86% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Mi</td>\n",
       "      <td>Polarized Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹799</td>\n",
       "      <td>33% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Dannilo</td>\n",
       "      <td>Others Spectacle Sunglasses (Free Size)</td>\n",
       "      <td>₹170</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Cat-eye S...</td>\n",
       "      <td>₹799</td>\n",
       "      <td>68% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>UV Protection, Polarized, Mirrored Retro Squar...</td>\n",
       "      <td>₹259</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Lee Topper</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹219</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Shield Sunglasses (Free Size)</td>\n",
       "      <td>₹719</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Aviator S...</td>\n",
       "      <td>₹949</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection, Mirrored Aviator Sunglasses (Fr...</td>\n",
       "      <td>₹269</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Aviator Sunglasses (58)</td>\n",
       "      <td>₹1,039</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Rectangul...</td>\n",
       "      <td>₹749</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Aviator S...</td>\n",
       "      <td>₹1,049</td>\n",
       "      <td>47% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Gradient, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹719</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Rich Club</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (54)</td>\n",
       "      <td>₹325</td>\n",
       "      <td>53% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>PHENOMENAL</td>\n",
       "      <td>UV Protection, Mirrored Retro Square Sunglasse...</td>\n",
       "      <td>₹350</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>GANSTA</td>\n",
       "      <td>UV Protection, Night Vision, Riding Glasses Av...</td>\n",
       "      <td>₹331</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Brand                                Product Description  \\\n",
       "0       VINCENT CHASE  by Lenskart Polarized, UV Protection Wayfarer ...   \n",
       "1        Silver Kartz           UV Protection Clubmaster Sunglasses (53)   \n",
       "2            Fastrack      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "3            Fastrack   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "4              PIRASO              UV Protection Aviator Sunglasses (54)   \n",
       "5              PIRASO          UV Protection Rectangular Sunglasses (52)   \n",
       "6              SUNBEE  UV Protection, Polarized Wayfarer Sunglasses (...   \n",
       "7       VINCENT CHASE  by Lenskart Polarized, UV Protection Cat-eye S...   \n",
       "8           Elligator                UV Protection Round Sunglasses (54)   \n",
       "9                SRPM             UV Protection Wayfarer Sunglasses (50)   \n",
       "10          New Specs   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "11      VINCENT CHASE  by Lenskart UV Protection Aviator Sunglasses (53)   \n",
       "12           Fastrack       UV Protection Aviator Sunglasses (Free Size)   \n",
       "13          New Specs          UV Protection Oval Sunglasses (Free Size)   \n",
       "14         PHENOMENAL         UV Protection Retro Square Sunglasses (53)   \n",
       "15      VINCENT CHASE  by Lenskart Polarized, UV Protection Aviator S...   \n",
       "16  SHAAH COLLECTIONS  UV Protection, Polarized, Mirrored Rectangular...   \n",
       "17     ROZZETTA CRAFT  Polarized, Riding Glasses Sports, Wrap-around ...   \n",
       "18              NuVew              UV Protection Aviator Sunglasses (57)   \n",
       "19      VINCENT CHASE  by Lenskart Polarized, UV Protection Aviator S...   \n",
       "20     ROZZETTA CRAFT  UV Protection Retro Square Sunglasses (Free Size)   \n",
       "21             PIRASO              UV Protection Aviator Sunglasses (58)   \n",
       "22         LIZA ANGEL  Riding Glasses, Night Vision Spectacle Sunglas...   \n",
       "23        john jacobs                UV Protection Round Sunglasses (53)   \n",
       "24             PIRASO              UV Protection Aviator Sunglasses (54)   \n",
       "25                 Mi           Polarized Aviator Sunglasses (Free Size)   \n",
       "26            Dannilo            Others Spectacle Sunglasses (Free Size)   \n",
       "27      VINCENT CHASE  by Lenskart Polarized, UV Protection Cat-eye S...   \n",
       "28             SUNBEE  UV Protection, Polarized, Mirrored Retro Squar...   \n",
       "29         Lee Topper   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "30           Fastrack        UV Protection Shield Sunglasses (Free Size)   \n",
       "31      VINCENT CHASE  by Lenskart Polarized, UV Protection Aviator S...   \n",
       "32     kingsunglasses  UV Protection, Mirrored Aviator Sunglasses (Fr...   \n",
       "33           Fastrack              UV Protection Aviator Sunglasses (58)   \n",
       "34      VINCENT CHASE  by Lenskart Polarized, UV Protection Rectangul...   \n",
       "35      VINCENT CHASE  by Lenskart Polarized, UV Protection Aviator S...   \n",
       "36           Fastrack  Gradient, UV Protection Wayfarer Sunglasses (F...   \n",
       "37          Rich Club         UV Protection Retro Square Sunglasses (54)   \n",
       "38         PHENOMENAL  UV Protection, Mirrored Retro Square Sunglasse...   \n",
       "39             GANSTA  UV Protection, Night Vision, Riding Glasses Av...   \n",
       "\n",
       "     Price Discount  \n",
       "0     ₹749  70% off  \n",
       "1     ₹281  81% off  \n",
       "2     ₹799  20% off  \n",
       "3     ₹639  20% off  \n",
       "4     ₹224  85% off  \n",
       "5     ₹252  90% off  \n",
       "6     ₹283  78% off  \n",
       "7     ₹949  52% off  \n",
       "8     ₹298  88% off  \n",
       "9     ₹211  83% off  \n",
       "10    ₹264  89% off  \n",
       "11  ₹1,229  38% off  \n",
       "12    ₹639  20% off  \n",
       "13    ₹207  79% off  \n",
       "14    ₹267  73% off  \n",
       "15    ₹999  50% off  \n",
       "16    ₹195  88% off  \n",
       "17    ₹499  75% off  \n",
       "18    ₹198  73% off  \n",
       "19    ₹949  52% off  \n",
       "20    ₹499  77% off  \n",
       "21    ₹323  87% off  \n",
       "22    ₹199  80% off  \n",
       "23  ₹3,325  33% off  \n",
       "24    ₹221  86% off  \n",
       "25    ₹799  33% off  \n",
       "26    ₹170  78% off  \n",
       "27    ₹799  68% off  \n",
       "28    ₹259  80% off  \n",
       "29    ₹219  78% off  \n",
       "30    ₹719  20% off  \n",
       "31    ₹949  52% off  \n",
       "32    ₹269  85% off  \n",
       "33  ₹1,039  20% off  \n",
       "34    ₹749  62% off  \n",
       "35  ₹1,049  47% off  \n",
       "36    ₹719  20% off  \n",
       "37    ₹325  53% off  \n",
       "38    ₹350  82% off  \n",
       "39    ₹331  83% off  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting 1st page scrapped data into DataFrame\n",
    "df=pd.DataFrame({'Brand':brand,'Product Description':pro_des,'Price':price,'Discount':dis})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2303c2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Wayfarer ...</td>\n",
       "      <td>₹749</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Silver Kartz</td>\n",
       "      <td>UV Protection Clubmaster Sunglasses (53)</td>\n",
       "      <td>₹281</td>\n",
       "      <td>81% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹799</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹639</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹224</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Aviator S...</td>\n",
       "      <td>₹999</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (55)</td>\n",
       "      <td>₹221</td>\n",
       "      <td>86% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹214</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Mirrored Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹379</td>\n",
       "      <td>74% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Aviator S...</td>\n",
       "      <td>₹949</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Brand Name                                Product Description Price  \\\n",
       "0    VINCENT CHASE  by Lenskart Polarized, UV Protection Wayfarer ...  ₹749   \n",
       "1     Silver Kartz           UV Protection Clubmaster Sunglasses (53)  ₹281   \n",
       "2         Fastrack      UV Protection Wayfarer Sunglasses (Free Size)  ₹799   \n",
       "3         Fastrack   UV Protection Rectangular Sunglasses (Free Size)  ₹639   \n",
       "4           PIRASO              UV Protection Aviator Sunglasses (54)  ₹224   \n",
       "..             ...                                                ...   ...   \n",
       "95   VINCENT CHASE  by Lenskart Polarized, UV Protection Aviator S...  ₹999   \n",
       "96          PIRASO              UV Protection Aviator Sunglasses (55)  ₹221   \n",
       "97  kingsunglasses                UV Protection Round Sunglasses (54)  ₹214   \n",
       "98       ROYAL SON            Mirrored Aviator Sunglasses (Free Size)  ₹379   \n",
       "99   VINCENT CHASE  by Lenskart Polarized, UV Protection Aviator S...  ₹949   \n",
       "\n",
       "   Discount  \n",
       "0   70% off  \n",
       "1   81% off  \n",
       "2   20% off  \n",
       "3   20% off  \n",
       "4   85% off  \n",
       "..      ...  \n",
       "95  50% off  \n",
       "96  86% off  \n",
       "97  78% off  \n",
       "98  74% off  \n",
       "99  52% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for scrapping data from next pages, defining Start & End of the page\n",
    "start= 0\n",
    "end= 3\n",
    "for page in range(start,end):\n",
    "    \n",
    "# Scrapping Brand detail from the next pages\n",
    "    for j in driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]'):\n",
    "        brand.append(j.text)\n",
    "    \n",
    "# Scrapping Product Description from the next pages\n",
    "    for k in driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa _2-ICcC\"]'):\n",
    "        pro1.append(k.text)\n",
    "        \n",
    "    for l in driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]'):\n",
    "        pro2.append(l.text)\n",
    "        \n",
    "    pro_des=pro1+pro2\n",
    "    \n",
    "# Scrapping Price from the next pages    \n",
    "    for m in driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]'):\n",
    "        price.append(m.text)\n",
    "    \n",
    "# Scrapping Discount from the next pages\n",
    "    for n in driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]'):\n",
    "        dis.append(n.text)  \n",
    "    \n",
    "    nxt_button=driver.find_elements(By.XPATH,\"//a[@class='ge-49M']\")#scraping the list of buttons from the page\n",
    "    try:\n",
    "        driver.get(nxt_button[page].get_attribute('href'))#getting the link from the list for next page\n",
    "    except:\n",
    "        print(\"Page not found\")\n",
    "\n",
    "# converting first 100 scrapped data into DataFrame\n",
    "df1=pd.DataFrame({'Brand Name':brand,'Product Description':pro_des, 'Price':price,'Discount':dis})\n",
    "df1.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dcd0ff",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. This task will be done in following steps:\n",
    "1. First get the webpage https://www.flipkart.com/\n",
    "2. Enter “iphone 11” in “Search” field . \n",
    "3. Then click the search button.\n",
    "\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc56365",
   "metadata": {},
   "source": [
    "# Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c3b79ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cb29b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webpage https://www.flipkart.com and directly login to product detail of iphone12 , as directed by Mentor \n",
    "driver.get('https://www.flipkart.com/apple-iphone-12-black-128-gb/product-reviews/itmf1f0a58f1ecd7?pid=MOBFWBYZK3HACR72&lid=LSTMOBFWBYZK3HACR72PX4KSA&marketplace=FLIPKART ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cd14b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>Wow superb camera phone\\nVery smooth speed and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>The brand is very trustworthy and i got genuin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>Awesome phone … value for money.. Happy with b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>Guys ,this is just Beast at Every Aspect of Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>Thanx flipkart for value super sale for short ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Classy product</td>\n",
       "      <td>I am a tech freak so you can trust my views -\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>3</td>\n",
       "      <td>Super!</td>\n",
       "      <td>Definitely an awesome product...an awesome exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Nice</td>\n",
       "      <td>An apple fan, I find iPhone 12 light, easier t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Stunning Phone from IPhone at Stunning price.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The most value for money iPhone of 2021 after ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating         Review Summary  \\\n",
       "0       5               Terrific   \n",
       "1       5       Perfect product!   \n",
       "2       5      Terrific purchase   \n",
       "3       5  Mind-blowing purchase   \n",
       "4       5     Highly recommended   \n",
       "..    ...                    ...   \n",
       "95      5         Classy product   \n",
       "96      3                 Super!   \n",
       "97      5                   Nice   \n",
       "98      5    Best in the market!   \n",
       "99      5              Brilliant   \n",
       "\n",
       "                                          Full Review  \n",
       "0   Wow superb camera phone\\nVery smooth speed and...  \n",
       "1   The brand is very trustworthy and i got genuin...  \n",
       "2   Awesome phone … value for money.. Happy with b...  \n",
       "3   Guys ,this is just Beast at Every Aspect of Co...  \n",
       "4   Thanx flipkart for value super sale for short ...  \n",
       "..                                                ...  \n",
       "95  I am a tech freak so you can trust my views -\\...  \n",
       "96  Definitely an awesome product...an awesome exp...  \n",
       "97  An apple fan, I find iPhone 12 light, easier t...  \n",
       "98  Stunning Phone from IPhone at Stunning price.\\...  \n",
       "99  The most value for money iPhone of 2021 after ...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate1=[]\n",
    "rate2=[]\n",
    "summary=[]\n",
    "f_rev=[]\n",
    "\n",
    "# for scrapping data from different pages to get first 100 reviews, defining Start & End of the page\n",
    "start=0\n",
    "end=15\n",
    "for page in range(start,end):\n",
    "    \n",
    "# Scrapping rating from different pages \n",
    "    for i in driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]'):\n",
    "        rate1.append(i.text)\n",
    "        \n",
    "    for i in driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1rdVr6 _1BLPMq\"]'):\n",
    "        rate2.append(i.text)\n",
    "    rate=rate1+rate2\n",
    "\n",
    "# Scrapping Review Summary from different pages\n",
    "    for j in driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]'):\n",
    "        summary.append(j.text)\n",
    "        \n",
    "# Scrapping Full Review from different pages\n",
    "    for k in driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]'):\n",
    "        f_rev.append(k.text)\n",
    "        \n",
    "#scraping the list of reviews data from different pages\n",
    "    nxt_button=driver.find_elements(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href')) #getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "\n",
    "# converting scrapped data for first 100 reviews into DataFrame\n",
    "review=pd.DataFrame({'Rating':rate,'Review Summary':summary,'Full Review':f_rev})\n",
    "review.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d4ab01",
   "metadata": {},
   "source": [
    "# Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field. You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515222bd",
   "metadata": {},
   "source": [
    "# Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8e3bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55ac3fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webpage https://www.flipkart.com/\n",
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4925b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing the log-in page window\n",
    "close=driver.find_element(By.XPATH,'/html/body/div[2]/div/div/button')\n",
    "close.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5427c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"sneakers\" in \"search for products\" field\n",
    "product=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "product.send_keys('sneakers')\n",
    "\n",
    "#clicking the Search button\n",
    "search=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de098839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VibeX</td>\n",
       "      <td>Mesh | Ultralightweight | Comfortable | Breath...</td>\n",
       "      <td>₹549</td>\n",
       "      <td>45% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RapidBox</td>\n",
       "      <td>Smash Wns v2 Z Sneakers For Women</td>\n",
       "      <td>₹630</td>\n",
       "      <td>36% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aadi</td>\n",
       "      <td>Super Stylish &amp; Trendy Combo Pack of 02 Pairs ...</td>\n",
       "      <td>₹374</td>\n",
       "      <td>81% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹1,542</td>\n",
       "      <td>55% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹544</td>\n",
       "      <td>71% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>RapidBox</td>\n",
       "      <td>Stylish &amp; Trendy Sneakers For Men</td>\n",
       "      <td>₹620</td>\n",
       "      <td>37% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Qitty</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹249</td>\n",
       "      <td>58% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Kzaara</td>\n",
       "      <td>Sneakers Sneakers For Men</td>\n",
       "      <td>₹299</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Layasa</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹499</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>sixXplus</td>\n",
       "      <td>Latest Women's Pack of 2 Stylish Slip-On Loafe...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Brand                                Product Description   Price  \\\n",
       "0      VibeX  Mesh | Ultralightweight | Comfortable | Breath...    ₹549   \n",
       "1   RapidBox                  Smash Wns v2 Z Sneakers For Women    ₹630   \n",
       "2       aadi  Super Stylish & Trendy Combo Pack of 02 Pairs ...    ₹374   \n",
       "3       PUMA                                   Sneakers For Men  ₹1,542   \n",
       "4     Chevit                                   Sneakers For Men    ₹544   \n",
       "..       ...                                                ...     ...   \n",
       "95  RapidBox                  Stylish & Trendy Sneakers For Men    ₹620   \n",
       "96     Qitty                                   Sneakers For Men    ₹249   \n",
       "97    Kzaara                          Sneakers Sneakers For Men    ₹299   \n",
       "98    Layasa                                   Sneakers For Men    ₹499   \n",
       "99  sixXplus  Latest Women's Pack of 2 Stylish Slip-On Loafe...    ₹499   \n",
       "\n",
       "   Discount  \n",
       "0   45% off  \n",
       "1   36% off  \n",
       "2   81% off  \n",
       "3   55% off  \n",
       "4   71% off  \n",
       "..      ...  \n",
       "95  37% off  \n",
       "96  58% off  \n",
       "97  70% off  \n",
       "98  50% off  \n",
       "99  50% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#introducing list for brand, product description, price & discount\n",
    "brand=[]\n",
    "prod1=[]\n",
    "prod2=[]\n",
    "price=[]\n",
    "discount=[]\n",
    "\n",
    "# for scrapping data from different pages to get first 100 reviews, defining Start & End of the page\n",
    "for page in range(0,3):\n",
    "    \n",
    "# Scrapping brand list from different pages \n",
    "    for i in driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]'):\n",
    "        brand.append(i.text)\n",
    "        \n",
    "# Scrapping product description from different pages \n",
    "    for j in driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]'):\n",
    "        prod1.append(j.text)\n",
    "        \n",
    "    for c in driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa _2-ICcC\"]'):\n",
    "        prod2.append(c.text)\n",
    "    prod=prod1+prod2\n",
    "    \n",
    "# Scrapping price from different pages \n",
    "    for k in driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]'):\n",
    "        price.append(k.text)\n",
    "        \n",
    "# Scrapping discount from different pages \n",
    "    for l in driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]'):\n",
    "        discount.append(l.text)\n",
    "        \n",
    "#scraping the required data from different pages\n",
    "    nxt_button=driver.find_elements(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "        \n",
    "# converting scrapped data for first 100 reviews into DataFrame\n",
    "Sneaker_data=pd.DataFrame({'Brand':brand,'Product Description':prod,'Price':price,'Discount':discount})\n",
    "Sneaker_data.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367bc5d",
   "metadata": {},
   "source": [
    "# Q7: Go to the link - https://www.myntra.com/shoes Set second Price filter and Color filter to “Black”, as shown in the below image. \n",
    "And then scrape First 100 shoes data you get.\n",
    "\n",
    "The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c4493",
   "metadata": {},
   "source": [
    "# Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80e77b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ff6fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webpage https://www.myntra.com/shoes\n",
    "driver.get('https://www.myntra.com/shoes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4b57a",
   "metadata": {},
   "source": [
    "Further steps can't be done since \"Inspect\" icon is not displayed on right-click. \n",
    "As per Mentor's direction, question is aborted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4572ba7",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa79920",
   "metadata": {},
   "source": [
    "# Q8: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. \n",
    "\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4cd187",
   "metadata": {},
   "source": [
    "# Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a41e1726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61bb3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webpage https://www.amazon.in/\n",
    "driver.get('https://www.amazon.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23eebd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering \"Laptop\" in search field\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "search.send_keys('Laptop')\n",
    "\n",
    "# clicking on the search button\n",
    "search_button=driver.find_element(By.XPATH,'//div[@class=\"nav-search-submit nav-sprite\"]')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40c7206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting CPU Type filter to “Intel Core i7” as required\n",
    "cpu_type=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[4]/li[12]/span/a/span')\n",
    "cpu_type.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b82a54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sponsored\\nSamsung Galaxy Book2 360 Intel 12th...</td>\n",
       "      <td>97,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sponsored\\nSamsung Galaxy Book2 Pro Intel 12th...</td>\n",
       "      <td>1,09,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Samsung Galaxy Book2 Intel 12th Gen core i7 39...</td>\n",
       "      <td>79,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...</td>\n",
       "      <td>87,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...</td>\n",
       "      <td>57,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ASUS VivoBook K15 OLED (2021), 15.6-inch FHD O...</td>\n",
       "      <td>82,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ASUS TUF Gaming F15 (2022), 15.6-inch (39.62 c...</td>\n",
       "      <td>1,09,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hp Pavilion 15 12Th Gen Intel Core I7 16Gb Sdr...</td>\n",
       "      <td>92,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HP Pavilion Plus, 12th Gen Intel Core i7 16GB ...</td>\n",
       "      <td>89,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ASUS Zenbook 14 OLED (2022), 14\" (35.56 cms) 2...</td>\n",
       "      <td>1,04,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title     Price\n",
       "0  Sponsored\\nSamsung Galaxy Book2 360 Intel 12th...    97,990\n",
       "1  Sponsored\\nSamsung Galaxy Book2 Pro Intel 12th...  1,09,990\n",
       "2  Samsung Galaxy Book2 Intel 12th Gen core i7 39...    79,490\n",
       "3  HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...    87,990\n",
       "4  ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...    57,490\n",
       "5  ASUS VivoBook K15 OLED (2021), 15.6-inch FHD O...    82,990\n",
       "6  ASUS TUF Gaming F15 (2022), 15.6-inch (39.62 c...  1,09,990\n",
       "7  Hp Pavilion 15 12Th Gen Intel Core I7 16Gb Sdr...    92,990\n",
       "8  HP Pavilion Plus, 12th Gen Intel Core i7 16GB ...    89,990\n",
       "9  ASUS Zenbook 14 OLED (2022), 14\" (35.56 cms) 2...  1,04,990"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrapping product title from the webpage\n",
    "title=[]\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"a-section a-spacing-none puis-padding-right-small s-title-instructions-style\"]')[0:10]:\n",
    "    title.append(i.text)\n",
    "    \n",
    "# Scrapping product price from the webpage\n",
    "price=[]\n",
    "for j in driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')[0:10]:\n",
    "    price.append(j.text)\n",
    "\n",
    "# converting scrapped data for first 10 Laptops data into DataFrame\n",
    "Laptop=pd.DataFrame({'Title':title,'Price':price}) \n",
    "Laptop.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0774f1f9",
   "metadata": {},
   "source": [
    "# Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida location. You have to scrape company name, No. of days ago when job was posted, Rating of the company. \n",
    "\n",
    "This task will be done in following steps:\n",
    "\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter “Data Scientist” and click on search button.\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter “Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2aeaa7",
   "metadata": {},
   "source": [
    "# Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "49a04d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a2b1418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webpage https://www.ambitionbox.com/\n",
    "driver.get('https://www.ambitionbox.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f14f2ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on \"Job\" options\n",
    "jobs=driver.find_element(By.XPATH,'/html/body/div[1]/nav[2]/div/ul/li[5]/a')\n",
    "jobs.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9e1052a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"Data Scientist\" in \"Skill, Designation, Companies\" field\n",
    "search=driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/input')\n",
    "search.send_keys('Data Scientist')\n",
    "\n",
    "# clicking the Search button\n",
    "search=driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/button')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "75ca6c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on location icon\n",
    "location=driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]')\n",
    "location.click()\n",
    "\n",
    "# entering \"Noida\" for location search\n",
    "place=driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[2]/input')\n",
    "place.send_keys('Noida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dd697066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the Search button\n",
    "search=driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[3]/div[1]/div[1]/div/label')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9ca83a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>No. of Days ago when job was posted</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBRE South Asia Pvt Ltd</td>\n",
       "      <td>8d ago</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>22hr ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Genpact</td>\n",
       "      <td>3d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ericsson India Global Services Pvt. Ltd.</td>\n",
       "      <td>18d ago</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>11d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>15d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>16d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ericsson India Global Services Pvt. Ltd.</td>\n",
       "      <td>28d ago</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SOPRA STERIA INDIA LIMITED</td>\n",
       "      <td>17d ago</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EXL Services.com ( I ) Pvt. Ltd.</td>\n",
       "      <td>17d ago</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Company Name  \\\n",
       "0                         CBRE South Asia Pvt Ltd   \n",
       "1                   GENPACT India Private Limited   \n",
       "2                                         Genpact   \n",
       "3        Ericsson India Global Services Pvt. Ltd.   \n",
       "4                   GENPACT India Private Limited   \n",
       "5  Optum Global Solutions (India) Private Limited   \n",
       "6  Optum Global Solutions (India) Private Limited   \n",
       "7        Ericsson India Global Services Pvt. Ltd.   \n",
       "8                      SOPRA STERIA INDIA LIMITED   \n",
       "9                EXL Services.com ( I ) Pvt. Ltd.   \n",
       "\n",
       "  No. of Days ago when job was posted Rating  \n",
       "0                              8d ago    4.3  \n",
       "1                            22hr ago    4.0  \n",
       "2                              3d ago    4.0  \n",
       "3                             18d ago    4.3  \n",
       "4                             11d ago    4.0  \n",
       "5                             15d ago    4.1  \n",
       "6                             16d ago    4.1  \n",
       "7                             28d ago    4.3  \n",
       "8                             17d ago    4.2  \n",
       "9                             17d ago    3.9  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrapping company name from the webpage\n",
    "company=[]\n",
    "for i in driver.find_elements(By.XPATH,'//p[@class=\"company body-medium\"]'): #scrapping company name from page\n",
    "    company.append(i.text)\n",
    "\n",
    "# Scrapping No. of days ago when job was posted from the webpage\n",
    "day=[]\n",
    "for j in driver.find_elements(By.XPATH,'//span[@class=\"body-small-l\"]'):\n",
    "    day.append(j.text)\n",
    "del day[1:20:2] \n",
    "    \n",
    "# Scrapping Rating of the Company from the webpage\n",
    "rating=[]\n",
    "for k in driver.find_elements(By.XPATH,'//span[@class=\"body-small\"]'):\n",
    "    rating.append(k.text)\n",
    "\n",
    "# converting scrapped data into DataFrame\n",
    "Jobs=pd.DataFrame({'Company Name':company, 'No. of Days ago when job was posted':day, 'Rating':rating})\n",
    "Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6835b7",
   "metadata": {},
   "source": [
    "# Q10: Write a python program to scrape the salary data for Data Scientist designation. You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary. \n",
    "\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and then click on “Data Scientist”. You have to scrape the data ticked in the above image.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "61698eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cffb49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webpage https://www.ambitionbox.com/\n",
    "driver.get('https://www.ambitionbox.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bf9093d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting salary option\n",
    "salary=driver.find_element(By.XPATH,'/html/body/div[1]/nav[2]/div/ul/li[3]/a')\n",
    "salary.click()\n",
    "\n",
    "# selecting first option from drop down menu\n",
    "browser=driver.find_element(By.XPATH,'/html/body/div[1]/nav[2]/div/ul/li[3]/div/ul/li[1]/div/div[2]/p')\n",
    "browser.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d7c1a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enterring 'Data Scientist' in Search job profile\n",
    "job=driver.find_element(By.XPATH,'//input[@class=\"tt-input\"]')\n",
    "job.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "76fc6c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting first option from drop down menu\n",
    "select=driver.find_element(By.XPATH,'/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/div/div/div[1]/div/div')\n",
    "select.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4ac499bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Total Salary Record</th>\n",
       "      <th>Average Salary</th>\n",
       "      <th>Minimum Salary</th>\n",
       "      <th>Maximum Salary</th>\n",
       "      <th>Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>based on 22 salaries</td>\n",
       "      <td>₹ 31.7L</td>\n",
       "      <td>₹ 25.0L</td>\n",
       "      <td>₹ 45.0L</td>\n",
       "      <td>3-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab Inbev</td>\n",
       "      <td>based on 53 salaries</td>\n",
       "      <td>₹ 19.7L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>₹ 25.5L</td>\n",
       "      <td>2-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Optum</td>\n",
       "      <td>based on 48 salaries</td>\n",
       "      <td>₹ 16.5L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 22.6L</td>\n",
       "      <td>2-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZS</td>\n",
       "      <td>based on 33 salaries</td>\n",
       "      <td>₹ 15.7L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "      <td>1-2 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>based on 109 salaries</td>\n",
       "      <td>₹ 15.2L</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>₹ 23.0L</td>\n",
       "      <td>2-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tiger Analytics</td>\n",
       "      <td>based on 65 salaries</td>\n",
       "      <td>₹ 14.7L</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "      <td>2-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Legato Health Technologies</td>\n",
       "      <td>based on 11 salaries</td>\n",
       "      <td>₹ 14.5L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "      <td>4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tredence</td>\n",
       "      <td>based on 12 salaries</td>\n",
       "      <td>₹ 14.1L</td>\n",
       "      <td>₹ 8.8L</td>\n",
       "      <td>₹ 17.5L</td>\n",
       "      <td>3 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UnitedHealth</td>\n",
       "      <td>based on 91 salaries</td>\n",
       "      <td>₹ 13.6L</td>\n",
       "      <td>₹ 8.0L</td>\n",
       "      <td>₹ 20.5L</td>\n",
       "      <td>2-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ford Motor</td>\n",
       "      <td>based on 21 salaries</td>\n",
       "      <td>₹ 13.5L</td>\n",
       "      <td>₹ 10.0L</td>\n",
       "      <td>₹ 18.0L</td>\n",
       "      <td>3-4 yrs experience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Company Name    Total Salary Record Average Salary  \\\n",
       "0                     Walmart   based on 22 salaries        ₹ 31.7L   \n",
       "1                    Ab Inbev   based on 53 salaries        ₹ 19.7L   \n",
       "2                       Optum   based on 48 salaries        ₹ 16.5L   \n",
       "3                          ZS   based on 33 salaries        ₹ 15.7L   \n",
       "4           Fractal Analytics  based on 109 salaries        ₹ 15.2L   \n",
       "5             Tiger Analytics   based on 65 salaries        ₹ 14.7L   \n",
       "6  Legato Health Technologies   based on 11 salaries        ₹ 14.5L   \n",
       "7                    Tredence   based on 12 salaries        ₹ 14.1L   \n",
       "8                UnitedHealth   based on 91 salaries        ₹ 13.6L   \n",
       "9                  Ford Motor   based on 21 salaries        ₹ 13.5L   \n",
       "\n",
       "  Minimum Salary Maximum Salary  Experience Required  \n",
       "0        ₹ 25.0L        ₹ 45.0L  3-4 yrs experience   \n",
       "1        ₹ 15.0L        ₹ 25.5L  2-4 yrs experience   \n",
       "2        ₹ 11.0L        ₹ 22.6L  2-4 yrs experience   \n",
       "3        ₹ 11.0L        ₹ 22.0L  1-2 yrs experience   \n",
       "4         ₹ 9.0L        ₹ 23.0L  2-4 yrs experience   \n",
       "5         ₹ 9.0L        ₹ 20.0L  2-4 yrs experience   \n",
       "6        ₹ 11.0L        ₹ 20.0L    4 yrs experience   \n",
       "7         ₹ 8.8L        ₹ 17.5L    3 yrs experience   \n",
       "8         ₹ 8.0L        ₹ 20.5L  2-4 yrs experience   \n",
       "9        ₹ 10.0L        ₹ 18.0L  3-4 yrs experience   "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrapping company name from the webpage\n",
    "comp=[]\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"company-info\"]/a'):\n",
    "    comp.append(i.text.split(\"\\n\"))\n",
    "    \n",
    "# merging sub lists into a single list and deleting even place items to get required data\n",
    "company=sum(comp,[])\n",
    "del company[1:20:2]\n",
    "\n",
    "# Scrapping total salary record from the webpage\n",
    "bsal=[]\n",
    "for a in driver.find_elements(By.XPATH,'//div[@class=\"sbold-list-header\"]/span'):\n",
    "    bsal.append(a.text.replace('(','').replace(')',''))\n",
    "\n",
    "# Scrapping Average salary from the webpage\n",
    "avgsal=[]\n",
    "for b in driver.find_elements(By.XPATH,'//p[@class=\"averageCtc\"]'): \n",
    "    avgsal.append(b.text)\n",
    "\n",
    "# Scrapping Minimum Salary & Maximum Salary from the webpage\n",
    "min_sal=[]\n",
    "max_sal=[]\n",
    "for c in driver.find_elements(By.XPATH,'//div[@class=\"value body-medium\"]'): \n",
    "    min_sal.append(c.text)\n",
    "    max_sal.append(c.text)\n",
    "del min_sal[1:20:2]\n",
    "del max_sal[0:20:2]\n",
    "\n",
    "# Scrapping Experience required data from the webpage\n",
    "exp=[]\n",
    "for d in driver.find_elements(By.XPATH,'//div[@class=\"sbold-list-header\"]'): \n",
    "    exp.append(d.text.split('('))\n",
    "    \n",
    "# merging sub lists into a single list and deleting even place items to get required data   \n",
    "req_exp=sum(exp,[]) \n",
    "del req_exp[1:20:2]\n",
    "\n",
    "# converting scrapped data into DataFrame\n",
    "df=pd.DataFrame({'Company Name':company,'Total Salary Record':bsal,'Average Salary':avgsal,'Minimum Salary':min_sal,'Maximum Salary':max_sal,'Experience Required':req_exp})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffad4078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
